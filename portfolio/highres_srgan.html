<!DOCTYPE html>

<html>
    <head>
      <title>Naoki Yokoyama</title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <!--Favicon-->
      <link rel="icon" type="../image/png" href="../img/favicon.png">
      <!--CSS Links-->
      <link rel="stylesheet" href="../css/navbar.css">
      <link rel="stylesheet" href="../css/bootstrap.min.css">
      <link rel="stylesheet" href="../css/styles.css">
      <!--Scripts-->
      <script type="text/javascript" src="../js/analytics.js"></script>
      <script type="text/javascript" src="../js/jquery-2.1.3.min.js"> </script>
      <script type="text/javascript" src="../js/portfolio.js"></script>
    </head>

    <body>
        
      <!--Navbar begins-->
      <div class="navbar2 hidden-xs">
        <div align="center">
          <ul>
            <a href="../index.html"><li>Home</li></a>
            <a href="../dlt/index.html"><li>Tutorials</li></a>
            <a href="../portfolio/index.html"><li>Portfolio</li></a>
            <a href="../about_me.html"><li>About Me</li></a>
            <a href="website.html"><li>This Site</li></a>
            <a href="../img/cv.pdf"><li>CV</li></a>
            <a href="../cs.html" id="spcl"><li>Contact</li></a>
          </ul>
        </div>
      </div>
      
      <div class="navbar navbar-inverse navbar-static-top visible-xs">
        <div class="container">
          <div class="navbar-header">
            <a href="#" class="navbar-brand">Naoki Yokoyama</a>
            <button class="navbar-toggle" data-toggle="collapse" data-target=".navHeaderCollapse">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
          <div class="collapse navbar-collapse navHeaderCollapse">
            <ul class="nav navbar-nav navbar-left">
              <li><a href="../index.html">Home</a></li>
              <li><a href="../dlt/index.html">Tutorials</a></li>
              <li><a href="../portfolio/index.html">Portfolio</a></li>
              <li class="dropdown">
                <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                <ul class="dropdown-menu">
                  <li><a href="../about_me.html">The Author</a></li>
                  <li><a href="portfolio/website.html">This Website</a></li>
                </ul>
              </li>
              <li><a href="../cs.html">Contact</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!--Navbar ends-->

      <div class="container">
        <div class="row">
          <div class="col-lg-1"></div>
          <div class="col-lg-10">
            <div class="panel panel-default">
              <div class="panel-body">
                <div class="page-header">
                  <center>
                    <h2>High Resolution SRGAN experiments using recent GAN research</h2>
                  </center>
                </div>
                <center>
                  <img src="../img/highres_srgan/style_gan_results.png" alt="" width="60%">
                  <br>
                  <i class="fontsize">Faces generated by NVIDIA's StyleGAN framework, trained on 8 Tesla V100 GPUs for 6 days 14 hours.</i>
                  <br>
                </center>

                <h3>Generating very realistic fake images</h3>
                <p class="fontsize">
                  The main aspects of NVIDIA's new GAN architecture allowing it to generate such realistic faces are:
                </p>
                <ul list-style-position='outside'>
                  <li class="fontsize">NVIDIA's new dataset, FFHQ, is comprised of 70,000 high quality 1024x1024 images of faces collected from Flickr and refined with Mechanical Turk. Their previous dataset, CelebA-HQ, which was built off of The Univ. of Hong Kong's CelebA dataset, only contained 30,000 images of celebrities, and was artificially augmented using a convolutional residual autoencoder (similar to RED-Net) and an SRGAN.</li>
                  <li class="fontsize">An 8-layer multilayer perceptron mapping the input vector to a different latent space, from which "styles" are sampled for each convolutional layer in the synthesis network of the generator. They claim that this latent space features better disentanglement and is perceptually more linear than the input space.</li>
                  <li class="fontsize">Layer-wise injection of input data and noise.</li>
                  <li class="fontsize">Like they did in their previous breakthrough in generating realistic faces with GANs, the synthesis component of the generator and the discriminator were gradually grown during training to progressively output/assess images of higher resolution, which speeds up and greatly stabilizes training.</li>
                </ul>

                <h3>Embedding the input latent code</h3>
                <p class="fontsize">
                  The generator of NVIDIA's StyleGAN is split into two networks, the <b>mapping network</b> and the <b>synthesis network</b>. Unlike traditional GANs, StyleGAN embeds the input code into an <b>intermediate latent space</b> which is done by the mapping network. The new resulting code is then <b>injected into each convolutional layer</b> of the synthesis network of the generator.
                </p>
                <center>
                  <img src="../img/highres_srgan/traditional_vs_stylegan.png" alt="" width="60%">
                </center>
                <ul list-style-position='outside'>
                  <li class="fontsize">The mapping network is an 8-layer multilayer perceptron with an output dimensionality of 512.</li>
                  <li class="fontsize">The input code, which lies in latent space <b>z</b>, is mapped to latent space <b>w</b>.</li>
                  <li class="fontsize">Since the training process enforces the sampling probability of the possible codes from <b>z</b> to match the corresponding density found in the training set, the paper argues that <b>some degree of entanglement between the factors of z is unavoidable.</b></li>
                  <li class="fontsize">However, since the latent space <b>w</b> is not restricted in the same way, it is allowed <b>and even encouraged</b> to be disentangled. The paper claims that this is because it should be easier for the network to decode disentangled representations than entangled ones.</li>
                  <li class="fontsize">Therefore, StyleGAN in theory generates a less entangled latent space <b>w</b>, even when the factors of variation are not known in advance.</li>
                  <li class="fontsize">The output of the mapping network is injected <b>into the output of each of the synthesis network's convolutional layers</b>, after being processed by a learned affine transform (i.e. a single fully connected layer) <b>specific to each convolutional layer.</b></li>
                  <li class="fontsize">As per NVIDIA's original paper on progressive GANs, they again reject the notion of that GANs suffer from covariate shift. Instead of using batch normalizaton, they use <b>adaptive instance normalization (AdaIn)</b> (Huang & Belongie, 2017), a faster and more efficient normalization technique. They use the aforementioned affine transformation to generate the scale and bias for the AdaIn operation.</li>
                </ul>

                <h3>Layer-wise input of styles</h3>
                <center>
                  <img src="../img/highres_srgan/style_layers.png" alt="" width="60%">
                  <p class="fontsize"><i>Mixing regularization: Visualization of different scales of style mixing. The latent codes for the generated images on the left are partially overwritten by the latent codes for the generated images on the top to generate the other images.</i></p>
                </center>
                <ul list-style-position='outside'>
                  <li class="fontsize">Since the code from the latent space <b>w</b> is inputted into each convolutional layer of the synthesis network, scale-specific modifications to the generated images can be made. Each of these inputs are referred to as "styles" since similar network architectures have already been used for style transfer, image-to-image translation, and domain mixture. </li>
                  <li class="fontsize">As each of the convolutional layers of the synthesis network is followed by a 2x upscaling layer, each style affects the resulting image at a different scale.</li>
                  <li class="fontsize">The AdaIn operation also serves to normalize feature maps and overwrite preceding style inputs.</li>
                  <li class="fontsize">To further localize the effects of the input styles, a portion of the input latent codes during training undergo <b>mixing regularization</b>, in which two latent <b>z</b> vectors are used to generate an image to attempt to fool the discriminator. This is done by using the two <b>z</b> vectors to generate two <b>w</b> vectors (<b>w1, w2</b>)and then generating a certain portion of the input styles using <b>w1</b> and the rest using <b>w2</b>. The crossover point where the source <b>w</b> vector for the input styles switches from <b>w1</b> to <b>w2</b> is randomly selected.</li>
                  <li class="fontsize">The localized effects of each style is illustrated in the above figure the demonstrates the results from mixture regularization.</li>
                </ul>

                <h3>Layer-wise stochastic inputs</h3>
                <center>
                  <img src="../img/highres_srgan/stochastic_results.png" alt="" width="60%">
                  <p class="fontsize"><i>Input noise govern acceptably stochastic aspects of the generated photos, such as specific placement of hair strands, pores, and reflections in the eyes. Images to the far right represent the standard deviation of each pixel over 100 different realizations of input noise at each layer, highlighting which aspects are affected by the noise. </i></p>
                </center>
                <ul list-style-position='outside'>
                  <li class="fontsize">Many aspects of human faces can be acceptably stochastic, such as the exact placement of hair/pores/freckles.</li>
                  <li class="fontsize">Since a <b>traditional GAN only accepts input through its initial input layer</b>, it must learn to <b>generate pseudo-random numbers from earlier activations whenever needed in order to produce these features</b>. This takes up part of the network's capacity to learn, and hiding the periodicity of the generated numbers is difficult, as shown by many repetitive patterns that are exhibited by traditional GANs.</li>
                  <li class="fontsize">By inputting <b>noise directly into each layer</b>, StyleGAN is relieved of this duty, and learns to use the noise to only adjust aspects of the image that can be slightly randomized, and does not exhibit the periodicity that a traditional GAN would.</li>
                  <li class="fontsize">Since a <b>fresh set of noise is provided to each layer</b>, the effects of each input noise is also tightly localized to its own layer, meaning that adjusting noise at a finer layer only affects finer details (ex. placement of skin pores) while coarser layers affect only coarse features (ex. large-scale hair curling and larger background features).</li>
                </ul>

                <h3>Evaluation of new mechanisms</h3>
                <center>
                  <img src="../img/highres_srgan/fid.png" alt="" width="80%">
                  <p class="fontsize"><i>FID (lower is better) between real and generated images resulting from applying various techniques introduced by the paper</i></p>
                </center>
                <ul list-style-position='outside'>
                  <li class="fontsize">The Fréchet Inception Distance is a metric obtained by using an InceptionV3 image classifier trained on ImageNet. The distributions between the activations generated by InceptionV3 each set of images (generated and real) are then compared using the Fréchet Distance. </li>
                </ul>

                <h3>References</h3>
                <ul list-style-position='outside'>
                  <li class="fontsize">Karras,  T.,  Laine,  S., & Aila,  T. A Style-Based Generator Architecture forGenerative Adversarial Networks. CoRR, abs/1812.04948, 2018.</li>
                  <li class="fontsize">X. Huang and S. J. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. CoRR, abs/1703.06868, 2017.</li>
                </ul>
                  
              </div>
            </div>
          </div>
        </div>
      </div>  
            
      <script src="../js/bootstrap.js"></script>
    </body>
</html>